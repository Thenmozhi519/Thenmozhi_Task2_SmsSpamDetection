{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfTvdjqaUuTk",
        "outputId": "6c4b6254-a0b8-4e69-afd5-63a26d849869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8825112107623319\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.89      0.98      0.94       965\n",
            "        spam       0.69      0.23      0.34       150\n",
            "\n",
            "    accuracy                           0.88      1115\n",
            "   macro avg       0.79      0.61      0.64      1115\n",
            "weighted avg       0.86      0.88      0.86      1115\n",
            "\n",
            "Confusion Matrix:\n",
            "[[950  15]\n",
            " [116  34]]\n"
          ]
        }
      ],
      "source": [
        "        import pandas as pd\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.neighbors import KNeighborsClassifier\n",
        "        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "        import re\n",
        "        import nltk\n",
        "        from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "        from gensim import corpora\n",
        "        import gensim\n",
        "        df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "        nltk.download('vader_lexicon')\n",
        "        sia = SentimentIntensityAnalyzer()\n",
        "        df['sentiment_score'] = df['v2'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "        texts = [[word for word in document.lower().split()] for document in df['v2']]\n",
        "        dictionary = corpora.Dictionary(texts)\n",
        "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "        lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=15)\n",
        "        df['topic_model'] = [lda_model.get_document_topics(doc) for doc in corpus]\n",
        "        df['v2'] = df['v2'].apply(lambda x: x.lower())\n",
        "        df['v2'] = df['v2'].apply(lambda x: re.sub(r'[^\\\\w\\\\s]', '', x))\n",
        "\n",
        "        X = df[['v2', 'sentiment_score', 'topic_model']]\n",
        "        y = df['v1']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train['v2'])\n",
        "        X_test_tfidf = vectorizer.transform(X_test['v2'])\n",
        "\n",
        "        knn = KNeighborsClassifier(n_neighbors=5)\n",
        "        knn.fit(X_train_tfidf, y_train)\n",
        "        y_pred = knn.predict(X_test_tfidf)\n",
        "\n",
        "        print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "        print('Classification Report:')\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print('Confusion Matrix:')\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "df['named_entities'] = df['v2'].apply(lambda x: [ent.text for ent in nlp(x).ents])\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "df['pos_tags'] = df['v2'].apply(lambda x: [pos for word, pos in pos_tag(word_tokenize(x))])\n",
        "\n",
        "df['v2'] = df['v2'].apply(lambda x: x.lower())\n",
        "df['v2'] = df['v2'].apply(lambda x: re.sub(r'[^\\\\w\\\\s]', '', x))\n",
        "\n",
        "X = df[['v2', 'named_entities', 'pos_tags']]\n",
        "y = df['v1']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train['v2'])\n",
        "X_test_tfidf = vectorizer.transform(X_test['v2'])\n",
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "rfc.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = rfc.predict(X_test_tfidf)\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La4zyAEfoSeC",
        "outputId": "3c3c33d5-a57a-4ff2-afc4-b3470cad4f5c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8941704035874439\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.91      0.98      0.94       965\n",
            "        spam       0.70      0.37      0.49       150\n",
            "\n",
            "    accuracy                           0.89      1115\n",
            "   macro avg       0.80      0.67      0.71      1115\n",
            "weighted avg       0.88      0.89      0.88      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.matutils import corpus2dense\n",
        "\n",
        "nmf_model = gensim.models.Nmf(corpus=corpus, num_topics=5, passes=15)\n",
        "df['topic_model_nmf'] = [nmf_model.get_document_topics(doc) for doc in corpus]\n",
        "\n"
      ],
      "metadata": {
        "id": "16akJuStpSIm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "X_train = df[['v2', 'named_entities', 'pos_tags']].iloc[X_train.index]\n",
        "X_test = df[['v2', 'named_entities', 'pos_tags']].iloc[X_test.index]\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "for col in ['named_entities', 'pos_tags']:\n",
        "    X_train[col] = X_train[col].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "    X_test[col] = X_test[col].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
        "\n",
        "X_train = encoder.fit_transform(X_train)\n",
        "X_test = encoder.transform(X_test)\n",
        "\n",
        "rfc.fit(X_train, y_train)\n",
        "y_pred_rfc = rfc.predict(X_test)\n",
        "print('Random Forest Accuracy:', accuracy_score(y_test, y_pred_rfc))\n",
        "print('Random Forest Classification Report:')\n",
        "print(classification_report(y_test, y_pred_rfc))\n",
        "print('Random Forest Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred_rfc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f32UIQ74r9A_",
        "outputId": "a527fbec-2d3a-4654-d715-99c9f4ab581b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.9067264573991032\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.90      1.00      0.95       965\n",
            "        spam       0.94      0.33      0.49       150\n",
            "\n",
            "    accuracy                           0.91      1115\n",
            "   macro avg       0.92      0.66      0.72      1115\n",
            "weighted avg       0.91      0.91      0.89      1115\n",
            "\n",
            "Random Forest Confusion Matrix:\n",
            "[[962   3]\n",
            " [101  49]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import re\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "df['sentiment_score'] = df['v2'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "texts = [[word for word in document.lower().split()] for document in df['v2']]\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=15)\n",
        "df['topic_model'] = [lda_model.get_document_topics(doc) for doc in corpus]\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "df['named_entities'] = df['v2'].apply(lambda x: [ent.text for ent in nlp(x).ents])\n",
        "\n",
        "df['pos_tags'] = df['v2'].apply(lambda x: [pos for word, pos in nltk.pos_tag(nltk.word_tokenize(x))])\n",
        "\n",
        "df['sentiment_score_tb'] = df['v2'].apply(lambda x: TextBlob(x).sentiment.polarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRKB8h9ksPaH",
        "outputId": "f9899f47-bd76-489e-e7a4-7e5aaca3e5de"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        import pandas as pd\n",
        "        import tensorflow as tf\n",
        "        from tensorflow.keras.layers import Embedding, Dense, Dropout, Flatten\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "        df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "\n",
        "        tokenizer = Tokenizer(num_words=5000)\n",
        "        tokenizer.fit_on_texts(df['v2'])\n",
        "\n",
        "        sequences = tokenizer.texts_to_sequences(df['v2'])\n",
        "        padded = pad_sequences(sequences, maxlen=200)\n",
        "\n",
        "        le = LabelEncoder()\n",
        "        df['v1'] = le.fit_transform(df['v1'])\n",
        "\n",
        "        X = padded\n",
        "        y = df['v1']\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
        "\n",
        "        loss, accuracy = model.evaluate(X_test, y_test)\n",
        "        print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8RngCQslkaK",
        "outputId": "b2a8bb3c-9ec8-462e-90f3-be6ba8baa69a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - accuracy: 0.8999 - loss: 0.2863 - val_accuracy: 0.9749 - val_loss: 0.0847\n",
            "Epoch 2/5\n",
            "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 77ms/step - accuracy: 0.9837 - loss: 0.0608 - val_accuracy: 0.9874 - val_loss: 0.0536\n",
            "Epoch 3/5\n",
            "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 43ms/step - accuracy: 0.9941 - loss: 0.0358 - val_accuracy: 0.9812 - val_loss: 0.0863\n",
            "Epoch 4/5\n",
            "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.9971 - loss: 0.0245 - val_accuracy: 0.9892 - val_loss: 0.0560\n",
            "Epoch 5/5\n",
            "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.9991 - loss: 0.0202 - val_accuracy: 0.9857 - val_loss: 0.0546\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9884 - loss: 0.0398\n",
            "Accuracy: 0.9856502413749695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        import pandas as pd\n",
        "        from sklearn.neighbors import KNeighborsClassifier\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "        import re\n",
        "\n",
        "        file_path = 'spam.csv'\n",
        "\n",
        "        df = pd.read_csv(file_path, encoding='latin-1')\n",
        "\n",
        "        df['v2'] = df['v2'].apply(lambda x: x.lower())\n",
        "        df['v2'] = df['v2'].apply(lambda x: re.sub(r'[^\\\\w\\\\s]', '', x))\n",
        "\n",
        "        X = df['v2']\n",
        "        y = df['v1']\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "        X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "        knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto')\n",
        "        knn.fit(X_train_tfidf, y_train)\n",
        "\n",
        "        y_pred = knn.predict(X_test_tfidf)\n",
        "\n",
        "        print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "        print('Classification Report:')\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print('Confusion Matrix:')\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "        param_grid = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance']}\n",
        "        grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')\n",
        "        grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "        print('Best Parameters:', grid_search.best_params_)\n",
        "        print('Best Score:', grid_search.best_score_)\n",
        "\n",
        "        best_knn = KNeighborsClassifier(**grid_search.best_params_)\n",
        "        best_knn.fit(X_train_tfidf, y_train)\n",
        "\n",
        "        y_pred_best = best_knn.predict(X_test_tfidf)\n",
        "\n",
        "        print('Accuracy:', accuracy_score(y_test, y_pred_best))\n",
        "        print('Classification Report:')\n",
        "        print(classification_report(y_test, y_pred_best))\n",
        "        print('Confusion Matrix:')\n",
        "        print(confusion_matrix(y_test, y_pred_best))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkr4wrxCj4yP",
        "outputId": "5942d256-5225-4eaf-f781-8b3301d19bc0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8825112107623319\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.89      0.98      0.94       965\n",
            "        spam       0.69      0.23      0.34       150\n",
            "\n",
            "    accuracy                           0.88      1115\n",
            "   macro avg       0.79      0.61      0.64      1115\n",
            "weighted avg       0.86      0.88      0.86      1115\n",
            "\n",
            "Confusion Matrix:\n",
            "[[950  15]\n",
            " [116  34]]\n",
            "Best Parameters: {'n_neighbors': 9, 'weights': 'distance'}\n",
            "Best Score: 0.8786182703970447\n",
            "Accuracy: 0.8977578475336323\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.91      0.98      0.94       965\n",
            "        spam       0.75      0.36      0.49       150\n",
            "\n",
            "    accuracy                           0.90      1115\n",
            "   macro avg       0.83      0.67      0.71      1115\n",
            "weighted avg       0.89      0.90      0.88      1115\n",
            "\n",
            "Confusion Matrix:\n",
            "[[947  18]\n",
            " [ 96  54]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}